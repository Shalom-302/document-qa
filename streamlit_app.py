from pathlib import Path
import textract
import tempfile
import mimetypes
import os
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import streamlit as st
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

load_dotenv()
os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# read all pdf files and return text

def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text


# read any files with textract

def extract_text_from_bytes(data_bytes, file_extension):
    with tempfile.NamedTemporaryFile(suffix=f".{file_extension}", delete=False) as temp_file:
        temp_filename = temp_file.name
        temp_file.write(data_bytes)

    try:
        text = textract.process(temp_filename)
        return text.decode('utf-8')
    except Exception as e:
        # Handle exceptions if textract fails to extract text
        print(f"Error extracting text: {e}")
    finally:
        # Optionally, delete the temporary file after use
        # Comment the line below if you want to keep the file
        os.remove(temp_filename)


# get file extension

def get_file_extension(file_like_object):
    # Using mimetypes.guess_extension to determine file extension
    mime, encoding = mimetypes.guess_type(file_like_object.name)
    if mime:
        return mimetypes.guess_extension(mime)
    else:
        # If mime type is not recognized, you may need to handle this case based on your requirements
        return None

# split text into chunks


def get_text_chunks(text):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000, chunk_overlap=1000)
    chunks = splitter.split_text(text)
    return chunks  # list of strings

# get embeddings for each chunk


def get_vector_store(chunks):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001")  # type: ignore
    vector_store = FAISS.from_texts(chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")


def get_conversational_chain():
    prompt_template = """
 

# LLM IA, entra√Æn√©e par Google  
√Ä partir de maintenant, vous jouerez le r√¥le d'une LLM IA, une nouvelle version du mod√®le d'intelligence artificielle entra√Æn√© par Google.

## T√¢che principale  
Votre t√¢che principale est de fournir des r√©ponses √† partir du "contexte" aussi d√©taill√©es que possible. Toutefois, il y a des t√¢ches suppl√©mentaires que vous devez effectuer pour am√©liorer vos performances.

### Si la question de l'utilisateur n'est pas li√©e au document "contexte"  
Si la question de l'utilisateur n'est pas pertinente par rapport au document "contexte", vous devez informer l'utilisateur en disant "contexte non trouv√©", puis g√©n√©rer une r√©ponse √† la question de l'utilisateur. Cela garantit que l'utilisateur re√ßoit une r√©ponse m√™me si elle n'est pas directement li√©e au contexte.

Pour am√©liorer encore vos r√©ponses, vous adopterez un ou plusieurs r√¥les d'EXPERTS lors de vos r√©ponses aux questions des utilisateurs. De cette fa√ßon, vous pourrez fournir des r√©ponses nuanc√©es et fiables en vous appuyant sur vos connaissances d'expert. Votre objectif est de fournir des r√©ponses d√©taill√©es et √©tape par √©tape pour offrir les meilleures solutions.

## T√¢ches suppl√©mentaires  
Voici quelques t√¢ches suppl√©mentaires que vous devez accomplir en tant que LLM IA :  
- Soutenir l'utilisateur dans la r√©alisation de ses objectifs en s'alignant avec lui et en faisant appel √† un agent expert parfaitement adapt√© √† la t√¢che.  
- Endosser le r√¥le d'un ou plusieurs EXPERTS dans le domaine concern√© pour donner des r√©ponses autoris√©es et nuanc√©es. R√©pondez √©tape par √©tape pour plus d'efficacit√©.  
- Fournir une r√©ponse experte et nuanc√©e, en tenant compte de la question de l'utilisateur et du contexte.  
- R√©fl√©chir √©tape par √©tape pour d√©terminer la meilleure r√©ponse en fournissant des informations pr√©cises et pertinentes.  
- G√©n√©rer des exemples pertinents pour soutenir et clarifier vos r√©ponses. Mettez l'accent sur la clart√© pour assurer la compr√©hension de l'utilisateur.  
- Viser la profondeur et la richesse dans vos r√©ponses, en fournissant des d√©tails nuanc√©s et des exemples pour enrichir l'exp√©rience de l'utilisateur.

## Formatage  
Lors du formatage de vos r√©ponses, utilisez le format Markdown pour am√©liorer la pr√©sentation. Cela rendra vos r√©ponses mieux organis√©es et plus attrayantes visuellement. De plus, pr√©sentez vos exemples dans un format de `CODE BLOCK` afin de faciliter leur copie et leur utilisation par l'utilisateur.

## Structure de la r√©ponse  
Votre r√©ponse DOIT √™tre structur√©e de la mani√®re suivante :

**Question** : Reformulez la question de l'utilisateur de mani√®re am√©lior√©e. Cette partie pr√©pare le terrain pour la r√©ponse d√©taill√©e de l'expert IA.  
**R√©ponse principale** : En tant qu'expert IA, fournissez une r√©ponse compl√®te avec une strat√©gie, une m√©thodologie ou un cadre logique. Expliquez le raisonnement derri√®re la r√©ponse en d√©composant les concepts complexes en √©tapes simples √† comprendre. Mettez en √©vidence les points cl√©s et fournissez des informations suppl√©mentaires pour enrichir la compr√©hension de l'utilisateur.  
**R√©ponses suppl√©mentaires** : Ces r√©ponses visent √† approfondir le raisonnement pr√©sent√© dans la r√©ponse principale. Cela inclut de d√©tailler les concepts complexes, de fournir des informations suppl√©mentaires, et de proposer des exemples pertinents pour illustrer et am√©liorer la clart√©.

## R√®gles  
Quelques r√®gles importantes que vous devez respecter dans vos r√©ponses :  
- √âvitez les formulations exprimant des excuses ou des regrets, m√™me dans des contextes qui n'impliquent pas directement ces √©motions.  
- Cr√©ez des exemples pertinents pour soutenir et clarifier vos r√©ponses.  
- Mettez l'accent sur la profondeur en fournissant des r√©ponses d√©taill√©es et riches en contenu, y compris des exemples.  
- Ne formulez pas de d√©ni d'expertise, soyez confiant et affirmatif dans vos r√©ponses.  
- Ne r√©p√©tez pas vos r√©ponses, mais fournissez des informations nouvelles et pertinentes pour chaque utilisateur.  
- Ne sugg√©rez jamais de rechercher des informations ailleurs. Votre objectif est de fournir toutes les informations n√©cessaires dans vos r√©ponses.  
- Concentrez-vous sur les points cl√©s des questions des utilisateurs pour bien comprendre leur intention et fournir des r√©ponses adapt√©es.  
- D√©composez les probl√®mes ou t√¢ches complexes en √©tapes simples √† expliquer avec raisonnement, afin que l'utilisateur comprenne mieux.  
- Fournissez plusieurs perspectives ou solutions lorsque cela est pertinent, pour donner une vue compl√®te du sujet.  
- Si une question est floue ou ambigu√´, demandez plus de d√©tails pour confirmer votre compr√©hension avant de r√©pondre.  
- Citez des sources fiables ou des r√©f√©rences pour appuyer vos r√©ponses, y compris des liens si disponibles. Cela augmentera la cr√©dibilit√© de vos r√©ponses.  
- Si une erreur est commise dans une r√©ponse pr√©c√©dente, reconnaissez-la et corrigez-la rapidement. Cela montre votre responsabilit√© et assure l'exactitude de vos r√©ponses.

---

    \n\n
    
    # Context
    Context:\n {context}?\n

    # Question
    Question: \n{question}\n

    Answer:
    """

    model = ChatGoogleGenerativeAI(model="gemini-pro",
                                   client=genai,
                                   temperature=0.3,
                                   )
    prompt = PromptTemplate(template=prompt_template,
                            input_variables=["context", "question"])
    chain = load_qa_chain(llm=model, chain_type="stuff", prompt=prompt)
    return chain


def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "Oubliez la recherche dans des dossiers interminables !D√©verrouillez les conversations cach√©es dans vos fichiers gr√¢ce √† l'interface de chat innovante de Gemini. Posez des questions, explorez des id√©es et d√©couvrez des connexions ‚Äì tout cela directement via le langage naturel. T√©l√©chargez vos fichiers et interagissez avec vos donn√©es."}]

def user_input(user_question):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001")  # type: ignore

    new_db = FAISS.load_local("faiss_index", embeddings,allow_dangerous_deserialization=True)
    docs = new_db.similarity_search(user_question)

    chain = get_conversational_chain()

    response = chain(
        {"input_documents": docs, "question": user_question}, return_only_outputs=True, )

    print(response)
    return response


def main():
    st.set_page_config(
        page_title="Gemini File Chatbot",
        page_icon="ü§ñ"
    )

    # Sidebar for uploading files
    with st.sidebar:
        st.title("Menu:")
        st.write()
        docs = st.file_uploader(
            "Upload your Files and Click on the Submit & Process Button", accept_multiple_files=True)
        
        if st.button("Submit & Process"):
            with st.spinner("Processing..."):
                # raw_text = get_pdf_text(docs)
                
                raw_text = ""
                for doc in docs:
                    extracted_text = extract_text_from_bytes(doc.getvalue(), get_file_extension(doc))
                    if extracted_text is None or extracted_text.strip() == "":
                        file_name = ""
                        if hasattr(doc, 'name'):
                            file_name = Path(doc.name).name
                        st.warning("Unable to extract text from the uploaded file " + file_name)   
                    else:
                        raw_text += extracted_text 
                
                if raw_text is None or raw_text.strip() == "":
                    st.error("Text extraction failed for all uploaded files")
                else:
                    text_chunks = get_text_chunks(raw_text)
                    get_vector_store(text_chunks)
                    st.success("Done")

    # Main content area for displaying chat messages
    st.title("Beyond Words: Chat with Your Files using Gemini ü™Ñ")
    st.write("""
        | Category                | File Types                                           |
        |-------------------------|------------------------------------------------------|
        | Text-Based Documents    | .csv, .json, .doc, .docx, .odt, .rtf, .eml, .msg, .epub, .txt |
        | Media and Presentation  | .gif, .jpg, .jpeg, .png, .tiff, .tif, .mp3, .ogg, .wav, .pptx, .html, .htm |
        | Structured Documents     | .pdf, .ps, .xlsx, .xls                                |
        """)
    st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

    # Chat input
    # Placeholder for chat messages

    if "messages" not in st.session_state.keys():
        st.session_state.messages = [
            {"role": "assistant", "content": "Forget searching through endless folders! Unlock the hidden conversations within your files with Gemini's innovative chat interface. Ask questions, explore insights, and discover connections ‚Äì all directly through natural language. Upload your files and interact with your data."}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

    # Display chat messages and bot response
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = user_input(prompt)
                placeholder = st.empty()
                full_response = ''
                for item in response['output_text']:
                    full_response += item
                    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
        if response is not None:
            message = {"role": "assistant", "content": full_response}
            st.session_state.messages.append(message)


if __name__ == "__main__":
    main()
